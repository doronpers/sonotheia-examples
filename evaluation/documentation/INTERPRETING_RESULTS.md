# Interpreting Results

This guide explains the metrics and outputs generated by the Audio Trust Harness.

## Core Philosophy: Deferral vs. Detection

This tool does **not** say "Real" or "Fake". It asks: *"Is this audio signal distinct and stable enough that a downstream model could reliably classify it?"*

- If **Yes** → `accept` (The audio is good quality for analysis)
- If **No** → `defer_to_review` (The audio is fragile; downstream results may be random guesses)
- If **Impossible** → `insufficient_evidence` (Clip is silence, noise, or broken)

---

## Key Metrics

### 1. Fragility Score

*Definition:* The **Coefficient of Variation (CV)** of measured indicators across perturbations.
$$ CV = \frac{\sigma}{\mu} $$
(Standard Deviation divided by Mean)

- **Range:** 0.0 to ∞ (typically 0.0 to 1.0)
- **Interpretation:**
  - `0.00 - 0.15`: **Stable**. Perturbations had little effect.
  - `0.15 - 0.30`: **Borderline**.
  - `> 0.30`: **Fragile**. The signal "collapsed" or shifted wildly when noise or compression was applied.

### 2. Deferral Actions

| Action | Meaning | Trigger / Threshold |
| :--- | :--- | :--- |
| **accept** | Audio is robust. | `fragility_score` < Threshold (default 0.3) |
| **defer_to_review** | Audio is unstable. | `fragility_score` > Threshold |
| **insufficient_evidence** | Signal quality is too poor. | `duration` < 0.5s OR `clipping` detected OR loading error |

---

## Reading the Audit Log (`.jsonl`)

Each line in the output JSONL represents one **slice** of audio processed with one specific **perturbation**.

**Key Fields:**

- `slice_index`: Which 10s chunk of the file.
- `perturbation_name`: `none` (original), `noise` (added Gaussian noise), etc.
- `indicators`: Raw values (e.g., spectral centroid, pitch stability).
- `fragility_score`: The calculated stability score (computed across all perturbations for this slice).

---

## Recommended Review Workflow

Do not treat `accept` as a "Human Authenticity" guarantee. It simply means the file is **technically sound**.

1. **Run Harness** on batch of files.
2. **Filter** for `defer_to_review`.
3. **Human Review**: Listen *only* to the flagged files.
    - Are they unintelligible?
    - Is there heavy background music?
    - Is it a recording-of-a-recording?
4. **Discard** files marked `insufficient_evidence` from training/analysis sets.
